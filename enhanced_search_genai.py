# -*- coding: utf-8 -*-
"""enhanced_search_genai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BigJOe4qcSfX03E5gi3-j_ekuX7okQpL
"""

! pip install -U sentence-transformers

!pip install langchain

!pip install chromadb==0.5.3

!pip install google-generativeai

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df=pd.read_csv('/content/drive/MyDrive/new_cleaned_text.csv')
df.head(1)

#TF-IDF(term frequency inverse document frequency) is a way to measure how important a word is in a document by considering
#how often it appears in that document and how rare it is across all documents.

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect = TfidfVectorizer(ngram_range=(1, 1),
                             lowercase=False,
                             preprocessor=None,
                             stop_words=None)

tfidf = tfidf_vect.fit_transform(df['clean_text'])

print(f"Shape of output (# of docs, # of unique vocabulary): {tfidf.shape}")

from sentence_transformers import SentenceTransformer

bert_vect = SentenceTransformer('all-MiniLM-L6-v2')

df['sbert_doc_embeddings'] = df['clean_text'].apply(bert_vect.encode)

df.head()

df.to_csv('/content/drive/MyDrive/cleaned_text_bert_1.csv', index=False)

import pandas as pd
df=pd.read_csv('/content/drive/MyDrive/cleaned_text_bert_1.csv')
df.head(1)

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document



text_splitter = RecursiveCharacterTextSplitter(separators=["\n\n","\n"," ",""],
    chunk_size=2000,#it splits for every 2000 characters
    chunk_overlap=100,
)

def split_text(text):
    doc = Document(page_content=text)
    return text_splitter.split_documents([doc])

# Apply chunking to each text in the DataFrame column
df['chunks'] = df['clean_text'].apply(split_text)

df.head(1)

df.to_csv('/content/drive/MyDrive/cleaned_text_chunks_1.csv', index=False)

import chromadb

# Define the directory where you want to store the database
persist_directory = "/content/drive/MyDrive/database_1"

# Initialize the ChromaDB client for persistent storage
client1 = chromadb.PersistentClient(path=persist_directory)

# Create or retrieve a collection
collection_1 = client1.get_or_create_collection(name="my_collection_1")

chunks_list = df["chunks"].tolist()

# Ensure all elements are strings
chunks_list = [str(chunk) for chunk in chunks_list]

collection_1.add(
    documents=chunks_list,
    ids=[f"id{i}" for i in range(len(chunks_list))]
)

results = collection_1.query(
    query_texts=["script info title english"],
    n_results=1
)

# Check if results are returned
if results['documents']:
    print("Embeddings successfully added.")
else:
    print("No embeddings found.")

collection_1.count()

print(len(collection_1.get()["ids"]))

import google.generativeai as genai

from google.generativeai.types import HarmCategory, HarmBlockThreshold

f = open(r"/content/drive/MyDrive/gemini_key.txt")
key = f.read()

genai.configure(api_key=key)

for m in genai.list_models():
    print(m.name)

audio_file = genai.upload_file(path="/content/drive/MyDrive/My Oh My(PagalWorld.com.so).mp3")

# Initialize a Gemini model appropriate for your use case.
model = genai.GenerativeModel(model_name="gemini-1.5-flash")

# Create the prompt.
prompt = "Please transcribe the following audio file into text. The audio contains a song with music that needs to be converted to written form."

# Pass the prompt and the audio file to Gemini.
response = model.generate_content([prompt, audio_file],safety_settings={
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
    }
)

# Print the transcript
text=response.text

text

print(type(text))

#chromadb uses eludian distance to measure the similarity

result = collection_1.query(
    query_texts=text,
    n_results=1
)

result['documents']

